{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Visual Transformer, Chihuahua vs Muffin","metadata":{}},{"cell_type":"markdown","source":"An implementation of a visual transformer trained using the [muffin-vs-chihuahua-image-classification](https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification) dataset","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"mnjw417zYu4l"}},{"cell_type":"code","source":"%pip install einops\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import Sequential\nfrom  tensorflow.keras import layers\nfrom tensorflow import einsum\nfrom einops import rearrange, repeat\nfrom einops.layers.tensorflow import Rearrange\nimport cv2","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"eSyhLJeYYu4o","outputId":"42015d61-d274-4523-f1a3-587a6433d1dc","execution":{"iopub.status.busy":"2023-08-02T23:04:23.957074Z","iopub.execute_input":"2023-08-02T23:04:23.957424Z","iopub.status.idle":"2023-08-02T23:04:44.504971Z","shell.execute_reply.started":"2023-08-02T23:04:23.957396Z","shell.execute_reply":"2023-08-02T23:04:44.503787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Function","metadata":{"id":"yqzLw9p-Yu4r"}},{"cell_type":"markdown","source":"This function will be used to make sure that input dimensions are represented as tuples (height, width) when needed.","metadata":{"id":"65mEU3vFYu4t"}},{"cell_type":"code","source":"def pair(t):\n    return t if isinstance(t, tuple) else (t, t)","metadata":{"id":"pbiL7h4rYu4u","execution":{"iopub.status.busy":"2023-08-02T23:04:44.507016Z","iopub.execute_input":"2023-08-02T23:04:44.507864Z","iopub.status.idle":"2023-08-02T23:04:44.513090Z","shell.execute_reply.started":"2023-08-02T23:04:44.507818Z","shell.execute_reply":"2023-08-02T23:04:44.512054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PreNorm Layer","metadata":{"id":"qSvqcMsIYu4v"}},{"cell_type":"markdown","source":"Custom layer representing the Pre-Normalization used within the transformer model. It takes the following parameter:\n* **fn**: The function to be applied to the normalized input. In the transformer, this function can be either the attention mechanism or the MLP.\n\nThe call method is where the actual pre-normalization takes place. It takes the following parameters:\n* **x**: The input tensor passed through the layer normalization.\n* **training**: Used to enable/disable dropout layers based on the training mode.","metadata":{"id":"9DBfMwCWYu4w"}},{"cell_type":"code","source":"class PreNorm(Layer):\n    def __init__(self, fn):\n        super(PreNorm, self).__init__()\n\n        self.norm = layers.LayerNormalization()\n        self.fn = fn\n\n    def call(self, x, training=True):\n        return self.fn(self.norm(x), training=training)","metadata":{"id":"P8EXNYDOYu4x","execution":{"iopub.status.busy":"2023-08-02T23:04:44.514192Z","iopub.execute_input":"2023-08-02T23:04:44.514483Z","iopub.status.idle":"2023-08-02T23:04:44.526665Z","shell.execute_reply.started":"2023-08-02T23:04:44.514457Z","shell.execute_reply":"2023-08-02T23:04:44.525628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multi-Layer Perceptron Layer","metadata":{"id":"r9gwpVR8Yu4y"}},{"cell_type":"markdown","source":"Custom layer representing the Multi-Layer Perceptron used within the transformer model. It takes the following parameters:\n* **dim**: The output dimension of the MLP layer, which is also the input and output dimension of each transformer block in the ViT model.\n* **hidden_dim**: The dimension of the hidden layer in the MLP. It determines the intermediate dimension between the input and output of the two dense layers.\n* **dropout**: The dropout rate applied to the output of both dense layers in the MLP. By default, it is set to 0.0 (no dropout).\n\nThe call method takes the following parameters:\n* **x**: The input tensor that is processed through the MLP.\n* **training**: Used to enable/disable dropout layers based on the training mode.\n\nThe GELU activation function has two implementations: the approximate version and the exact version. The approximate flag can be set to True to use the approximate GELU. By default, the exact GELU is used.","metadata":{"id":"3kjFqX-KYu4z"}},{"cell_type":"code","source":"class MLP(Layer):\n    def __init__(self, dim, hidden_dim, dropout=0.0):\n        super(MLP, self).__init__()\n\n        def GELU():\n            def gelu(x, approximate=False):\n                if approximate:\n                    coeff = tf.cast(0.044715, x.dtype)\n                    return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n                else:\n                    return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n\n            return layers.Activation(gelu)\n\n        self.net = Sequential([\n            layers.Dense(units=hidden_dim),\n            GELU(),\n            layers.Dropout(rate=dropout),\n            layers.Dense(units=dim),\n            layers.Dropout(rate=dropout)\n        ])\n\n    def call(self, x, training=True):\n        return self.net(x, training=training)","metadata":{"id":"8-IlpbXEYu40","execution":{"iopub.status.busy":"2023-08-02T23:04:44.528977Z","iopub.execute_input":"2023-08-02T23:04:44.529967Z","iopub.status.idle":"2023-08-02T23:04:44.541364Z","shell.execute_reply.started":"2023-08-02T23:04:44.529917Z","shell.execute_reply":"2023-08-02T23:04:44.540170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attention Layer","metadata":{"id":"SdjWK0k-Yu41"}},{"cell_type":"markdown","source":"Custom layer representing the scaled dot-product attention mechanism used in the transformer model. It takes the following parameters:\n* **dim**: The input and output dimension of the attention layer.\n* **heads**: The number of attention heads.\n* **dim_head**: The dimension of each attention head.\n* **dropout**: The dropout rate applied to the attention weights.\n\nThe call method is where the actual attention calculation takes place. It takes the following parameters:\n* **x**: The input tensor passed through the layer normalization.\n* **training**: Used to enable/disable dropout layers based on the training mode.\n\nThe Attention class uses two sub-layers:\n* **self.attend**: This is the softmax activation function, which calculates the attention weights using the dot products between queries and keys. The softmax ensures that the attention weights are normalized and sum up to 1.\n* **self.to_qkv**: This is a linear transformation layer without biases, projecting the input tensor **x** to the queries, keys, and values. The output dimension of this layer is inner_dim * 3, where inner_dim is the dimension of queries, keys, and values for multi-head attention.\n* **self.to_out**: This is a list of layers used to project the attention output back to the original input dimension dim, followed by dropout. If project_out is False (which happens when there is only one attention head and its dimension is the same as dim), this layer is an empty list, indicating that no additional projection is needed.","metadata":{"id":"mvKOaZnIYu41"}},{"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n        super(Attention, self).__init__()\n        inner_dim = dim_head * heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = layers.Softmax()\n        self.to_qkv = layers.Dense(units=inner_dim * 3, use_bias=False)\n\n        if project_out:\n            self.to_out = [\n                layers.Dense(units=dim),\n                layers.Dropout(rate=dropout)\n            ]\n        else:\n            self.to_out = []\n\n        self.to_out = Sequential(self.to_out)\n\n    def call(self, x, training=True):\n        qkv = self.to_qkv(x)\n        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n\n        # dots = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) * self.scale\n        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n        attn = self.attend(dots)\n\n        # x = tf.matmul(attn, v)\n        x = einsum('b h i j, b h j d -> b h i d', attn, v)\n        x = rearrange(x, 'b h n d -> b n (h d)')\n        x = self.to_out(x, training=training)\n\n        return x","metadata":{"id":"MB_iyPGeYu41","execution":{"iopub.status.busy":"2023-08-02T23:04:44.543298Z","iopub.execute_input":"2023-08-02T23:04:44.543733Z","iopub.status.idle":"2023-08-02T23:04:44.553427Z","shell.execute_reply.started":"2023-08-02T23:04:44.543698Z","shell.execute_reply":"2023-08-02T23:04:44.552406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Layer","metadata":{"id":"Ux7Gy6nHYu42"}},{"cell_type":"markdown","source":"Custom layer representing the core building block of the transformer model. It takes the following parameters:\n* **dim**: The output dimension of the transformer block.\n* **depth**: The number of transformer blocks to stack.\n* **heads**: The number of attention heads in the multi-head attention mechanism.\n* **dim_head**: The dimension of each attention head. The total dimension of queries, keys, and values will be dim_head * heads.\n* **mlp_dim**: The dimension of the hidden layer in the MLP used within the transformer block.\n* **dropout**: The dropout rate applied to the output of both attention and MLP layers in the transformer block.\n\nThe call method is where the input tensor x is processed through the transformer blocks. It takes the following parameters:\n* **x**: The input tensor passed through the layer normalization.\n* **training**: Used to enable/disable dropout layers based on the training mode.","metadata":{"id":"NzPl2ovbYu42"}},{"cell_type":"code","source":"class Transformer(Layer):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.0):\n        super(Transformer, self).__init__()\n\n        self.layers = []\n\n        for _ in range(depth):\n            self.layers.append([\n                PreNorm(Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n                PreNorm(MLP(dim, mlp_dim, dropout=dropout))\n            ])\n\n    def call(self, x, training=True):\n        for attn, mlp in self.layers:\n            x = attn(x, training=training) + x\n            x = mlp(x, training=training) + x\n\n        return x","metadata":{"id":"ziMqiwOHYu42","execution":{"iopub.status.busy":"2023-08-02T23:04:44.554943Z","iopub.execute_input":"2023-08-02T23:04:44.555528Z","iopub.status.idle":"2023-08-02T23:04:44.568741Z","shell.execute_reply.started":"2023-08-02T23:04:44.555488Z","shell.execute_reply":"2023-08-02T23:04:44.568061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visual Transformer Model","metadata":{"id":"RErd-HmNYu42"}},{"cell_type":"markdown","source":"Custom model representing the Vision Transformer model. It takes the following parameters:\n* **image_size**: The size of the input image. If you have rectangular images, the image_size should be the maximum of the width and height to maintain aspect ratio.\n* **patch_size**: The size of each patch in the image. The image_size must be divisible by patch_size. \n* **num_classes**: The number of classes to classify. It represents the output dimension of the final classification layer.\n* **dim**: The output dimension of the transformer block. This is usually the hidden dimension of the transformer.\n* **depth**: The number of transformer blocks to stack.\n* **heads**: The number of attention heads in the multi-head attention mechanism.\n* **mlp_dim**: The dimension of the hidden layer in the MLP used within the transformer block.\n* **pool**: The pooling type for obtaining the final classification. It can be either 'cls' (using the class token) or 'mean' (using mean pooling).\n* **dim_head**: The dimension of each attention head. The total dimension of queries, keys, and values will be dim_head * heads.\n* **dropout**: The dropout rate applied to the output of both attention and MLP layers in the transformer block. By default, it is set to 0.0 (no dropout).\n* **emb_dropout**: The embedding dropout rate. It is applied to the output of the patch embeddings and the positional embeddings.\n\nThe call method is the forward pass of the model. It takes the following parameters:\n* **img**: This is the input image tensor that will be passed through the ViT model.\n* **training**: This is a boolean argument that controls whether the model is in training mode or not. It is used to enable or disable certain operations, such as dropout layers, based on the training status. By default, it is set to True, indicating that the model is in training mode.\n\nThe shape of **img** should be (batch_size, image_height, image_width, num_channels), where:\n* batch_size: The number of input images in a batch.\n* image_height: The height of the input image.\n* image_width: The width of the input image.\n* num_channels: The number of channels in the input image (e.g., 3 for RGB images).","metadata":{"id":"bmd98STlYu42"}},{"cell_type":"code","source":"class ViT(Model):\n    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim,\n                 pool='cls', dim_head=64, dropout=0.0, emb_dropout=0.0):\n        super(ViT, self).__init__()\n\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.patch_embedding = Sequential([\n            Rearrange('b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n            layers.Dense(units=dim)\n        ], name='patch_embedding')\n\n        self.pos_embedding = tf.Variable(initial_value=tf.random.normal([1, num_patches + 1, dim]))\n        self.cls_token = tf.Variable(initial_value=tf.random.normal([1, 1, dim]))\n        self.dropout = layers.Dropout(rate=emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n\n        self.mlp_head = Sequential([\n            layers.LayerNormalization(),\n            layers.Dense(units=num_classes)\n        ], name='mlp_head')\n\n    def call(self, img, training=True, **kwargs):\n        x = self.patch_embedding(img)\n        b, n, d = x.shape\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n        x = tf.concat([cls_tokens, x], axis=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x, training=training)\n\n        x = self.transformer(x, training=training)\n\n        if self.pool == 'mean':\n            x = tf.reduce_mean(x, axis=1)\n        else:\n            x = x[:, 0]\n\n        x = self.mlp_head(x)\n\n        return x","metadata":{"id":"DZT-kOidYu43","execution":{"iopub.status.busy":"2023-08-02T23:04:44.570288Z","iopub.execute_input":"2023-08-02T23:04:44.570764Z","iopub.status.idle":"2023-08-02T23:04:44.583841Z","shell.execute_reply.started":"2023-08-02T23:04:44.570729Z","shell.execute_reply":"2023-08-02T23:04:44.583074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"def read_and_preprocess_images(image_folder, target_size):\n    images = []\n    labels = []\n\n    for label, class_name in enumerate(os.listdir(image_folder)):\n        class_folder = os.path.join(image_folder, class_name)\n        for image_name in os.listdir(class_folder):\n            image_path = os.path.join(class_folder, image_name)\n            image = cv2.imread(image_path)\n            image = cv2.resize(image, target_size)  # Resize all images to a target size\n            image = image.astype(np.float32) / 255.0  # Normalize the image pixel values\n            images.append(image)\n            labels.append(label)\n\n    return np.array(images), np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T23:04:44.585005Z","iopub.execute_input":"2023-08-02T23:04:44.585267Z","iopub.status.idle":"2023-08-02T23:04:44.600021Z","shell.execute_reply.started":"2023-08-02T23:04:44.585243Z","shell.execute_reply":"2023-08-02T23:04:44.599279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the image size for ViT\nimage_size = (128, 128)  # You can choose an appropriate size based on the images\n\n# Define the paths to the train and test folders\ntrain_folder = \"/kaggle/input/muffin-vs-chihuahua-image-classification/train\"\ntest_folder = \"/kaggle/input/muffin-vs-chihuahua-image-classification/test\"\n\n# Read and preprocess images from the train and test folders\ntrain_images, train_labels = read_and_preprocess_images(train_folder, image_size)\ntest_images, test_labels = read_and_preprocess_images(test_folder, image_size)\n\n# Convert labels to one-hot encoded vectors\nnum_classes = 2  # 2 classes: chihuahua and muffin\ntrain_labels = tf.one_hot(train_labels, depth=num_classes)\ntest_labels = tf.one_hot(test_labels, depth=num_classes)\n\n# Create a dataset from the training images and labels\nbatch_size = 32  # You can choose an appropriate batch size based on your memory capacity\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\ntrain_dataset = train_dataset.shuffle(buffer_size=train_images.shape[0]).batch(batch_size)","metadata":{"id":"1kDHR0-BYu44","execution":{"iopub.status.busy":"2023-08-02T23:04:44.601004Z","iopub.execute_input":"2023-08-02T23:04:44.601516Z","iopub.status.idle":"2023-08-02T23:06:44.607748Z","shell.execute_reply.started":"2023-08-02T23:04:44.601487Z","shell.execute_reply":"2023-08-02T23:06:44.606609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"z425DILGYu43"}},{"cell_type":"code","source":"def train_model(model, train_dataset, epochs, batch_size):\n    # Create a SparseCategoricalCrossentropy loss function\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    # Create an Adam optimizer with a learning rate of 1e-4\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\n    # Define a train_step function, which will be called in each training iteration\n    def train_step(images, labels):\n        # Create a GradientTape to compute gradients for trainable variables\n        with tf.GradientTape() as tape:\n            # Make predictions using the model with training=True to enable dropout, etc.\n            predictions = model(images, training=True)\n            # Compute the loss between the predicted values and the actual labels\n            loss = loss_fn(labels, predictions)\n\n        # Calculate gradients of the loss with respect to trainable variables\n        gradients = tape.gradient(loss, model.trainable_variables)\n        # Apply the gradients to update the model's trainable variables\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        # Return the loss for this training step\n        return loss\n\n    # Training loop: run for the specified number of epochs\n    for epoch in range(epochs):\n        total_loss = 0\n        num_batches = 0\n\n        # Print the number of epochs\n        print(f'Epoch {epoch + 1}/{epochs}')\n\n        # Iterate over the training dataset in batches\n        for batch_images, batch_labels in train_dataset:\n            # Perform a training step for the current batch and get the loss\n            # Convert one-hot encoded labels to integer labels\n            labels = tf.argmax(batch_labels, axis=-1)\n            loss = train_step(batch_images, labels)\n            # Accumulate the loss for this epoch\n            total_loss += loss\n            num_batches += 1\n\n        # Calculate the average loss for this epoch\n        average_loss = total_loss / num_batches\n        # Print the average loss of the model\n        print(f'Loss: {average_loss:.4f}')\n\n    # Return the trained model\n    return model","metadata":{"id":"odnEgGK2Yu43","execution":{"iopub.status.busy":"2023-08-02T23:06:44.610554Z","iopub.execute_input":"2023-08-02T23:06:44.610867Z","iopub.status.idle":"2023-08-02T23:06:44.618890Z","shell.execute_reply.started":"2023-08-02T23:06:44.610839Z","shell.execute_reply":"2023-08-02T23:06:44.618048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the ViT model\nvit = ViT(\n    image_size=image_size,\n    patch_size=16,  # You can choose an appropriate patch size based on the image size and complexity\n    num_classes=num_classes,\n    dim=512,\n    depth=6,\n    heads=8,\n    mlp_dim=1024,\n    dropout=0.1,\n    emb_dropout=0.1\n)\n\n# Train the model on the new dataset\nepochs = 10\nvit = train_model(vit, train_dataset, epochs, batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T23:06:44.619985Z","iopub.execute_input":"2023-08-02T23:06:44.620912Z","iopub.status.idle":"2023-08-03T00:14:20.128503Z","shell.execute_reply.started":"2023-08-02T23:06:44.620883Z","shell.execute_reply":"2023-08-03T00:14:20.127359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Model","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test dataset\ntest_predictions = vit(test_images, training=False)\ntest_accuracy = tf.reduce_mean(tf.keras.metrics.categorical_accuracy(test_labels, test_predictions))\nprint(f'Test Accuracy: {test_accuracy.numpy():.4f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-03T00:14:20.130272Z","iopub.execute_input":"2023-08-03T00:14:20.130598Z","iopub.status.idle":"2023-08-03T00:14:53.360123Z","shell.execute_reply.started":"2023-08-03T00:14:20.130572Z","shell.execute_reply":"2023-08-03T00:14:53.358910Z"},"trusted":true},"execution_count":null,"outputs":[]}]}